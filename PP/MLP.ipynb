{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP \n",
    "\n",
    "### Что такое MLP?\n",
    "\n",
    "MLP в машинном обучении обозначает многослойный персептрон (Multilayer Perceptron). Это вид искусственной нейронной сети, состоящей из нескольких слоев нейронов (перцептронов), включая входной слой, скрытые слои и выходной слой. MLP является одним из основных типов нейронных сетей, используемых в задачах классификации и регрессии.\n",
    "\n",
    "Основные компоненты MLP:\n",
    "\n",
    "1. Входной слой (Input Layer): Нейроны этого слоя представляют входные признаки данных. Количество нейронов в этом слое равно числу признаков в наборе данных.\n",
    "\n",
    "2. Скрытые слои (Hidden Layers): Эти слои находятся между входным и выходным слоями. Каждый нейрон в скрытых слоях получает взвешенные входы от предыдущего слоя, применяет активационную функцию, и передает выход следующему слою. Скрытые слои позволяют моделировать более сложные нелинейные зависимости в данных.\n",
    "\n",
    "3. Выходной слой (Output Layer): Нейроны этого слоя представляют собой выходы модели. Количество нейронов в этом слое зависит от типа задачи (например, один нейрон для задачи регрессии, несколько для задачи классификации).\n",
    "\n",
    "Каждое соединение между нейронами имеет вес, который определяет силу влияния одного нейрона на другой. В процессе обучения веса настраиваются таким образом, чтобы минимизировать ошибку модели на тренировочных данных.\n",
    "\n",
    "MLP использует метод обратного распространения ошибки (backpropagation) для обучения. Этот метод заключается в последовательном распространении входных данных через сеть для получения прогнозов, вычислении ошибки, а затем обновлении весов с целью уменьшения ошибки.\n",
    "\n",
    "MLP является мощным инструментом в машинном обучении и может быть применен в различных областях, включая обработку изображений, распознавание речи, и многие другие."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем датасеты\n",
    "data_cls = pd.read_csv('csgo.csv')\n",
    "data_reg = pd.read_csv('wines.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализация классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_cls:\n",
    "    def __init__(self, layer_sizes, activation_functions):\n",
    "        # Инициализация сети с указанием размеров слоёв и функций активации\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activation_functions = activation_functions\n",
    "        # Инициализация весов случайными значениями\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(layer_sizes[:-1], layer_sizes[1:])]\n",
    "        # Инициализация смещений случайными значениями\n",
    "        self.biases = [np.random.randn(y, 1) for y in layer_sizes[1:]]\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        # Сигмоида\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def sigmoid_derivative(self, z):\n",
    "        # Производная сигмоиды\n",
    "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
    "\n",
    "    def tanh(self, z):\n",
    "        # Гиперболический тангенс\n",
    "        return np.tanh(z)\n",
    "\n",
    "    def tanh_derivative(self, z):\n",
    "        # Производная гиперболического тангенса\n",
    "        return 1 - np.tanh(z) ** 2\n",
    "\n",
    "    def relu(self, z):\n",
    "        # ReLU (Rectified Linear Unit)\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def relu_derivative(self, z):\n",
    "        # Производная ReLU\n",
    "        return (z > 0) * 1\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        # Прямое распространение\n",
    "        for b, w, activation in zip(self.biases, self.weights, self.activation_functions):\n",
    "            # Выбор функции активации и вычисление активации для текущего слоя\n",
    "            if activation == 'sigmoid':\n",
    "                input_data = self.sigmoid(np.dot(w, input_data) + b)\n",
    "            elif activation == 'tanh':\n",
    "                input_data = self.tanh(np.dot(w, input_data) + b)\n",
    "            elif activation == 'relu':\n",
    "                input_data = self.relu(np.dot(w, input_data) + b)\n",
    "        return input_data\n",
    "\n",
    "    def backpropagation(self, input_data, target):\n",
    "        # Инициализация градиентов для весов и смещений нулями\n",
    "        gradient_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        gradient_w = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "        # Прямое распространение с сохранением активаций и взвешенных сумм (z)\n",
    "        activation = input_data\n",
    "        activations = [input_data]\n",
    "        zs = []\n",
    "        for b, w, activation_function in zip(self.biases, self.weights, self.activation_functions):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            if activation_function == 'sigmoid':\n",
    "                activation = self.sigmoid(z)\n",
    "            elif activation_function == 'tanh':\n",
    "                activation = self.tanh(z)\n",
    "            elif activation_function == 'relu':\n",
    "                activation = self.relu(z)\n",
    "            activations.append(activation)\n",
    "\n",
    "        # Обратное распространение\n",
    "        # Вычисление ошибки на выходном слое\n",
    "        delta = self.cost_derivative(activations[-1], target) * self.get_activation_derivative(activations[-1], self.activation_functions[-1])\n",
    "        gradient_b[-1] = delta\n",
    "        gradient_w[-1] = np.dot(delta, activations[-2].T)\n",
    "\n",
    "        # Вычисление ошибки на скрытых слоях\n",
    "        for l in range(2, len(self.layer_sizes)):\n",
    "            z = zs[-l]\n",
    "            activation_derivative = self.get_activation_derivative(z, self.activation_functions[-l])\n",
    "            delta = np.dot(self.weights[-l + 1].T, delta) * activation_derivative\n",
    "            gradient_b[-l] = delta\n",
    "            gradient_w[-l] = np.dot(delta, activations[-l - 1].T)\n",
    "\n",
    "        return gradient_b, gradient_w\n",
    "\n",
    "    def get_activation_derivative(self, z, activation_function):\n",
    "        # Получение производной функции активации\n",
    "        if activation_function == 'sigmoid':\n",
    "            return self.sigmoid_derivative(z)\n",
    "        elif activation_function == 'tanh':\n",
    "            return self.tanh_derivative(z)\n",
    "        elif activation_function == 'relu':\n",
    "            return self.relu_derivative(z)\n",
    "\n",
    "    def cost_derivative(self, output_activations, target):\n",
    "        # Производная функции стоимости (ошибки)\n",
    "        return output_activations - target\n",
    "\n",
    "    def update_parameters(self, mini_batch, learning_rate):\n",
    "        # Обновление параметров сети (весов и смещений) на основе градиентов\n",
    "        sum_gradient_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        sum_gradient_w = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "        for x, y in mini_batch:\n",
    "            delta_gradient_b, delta_gradient_w = self.backpropagation(x, y)\n",
    "            sum_gradient_b = [nb + dnb for nb, dnb in zip(sum_gradient_b, delta_gradient_b)]\n",
    "            sum_gradient_w = [nw + dnw for nw, dnw in zip(sum_gradient_w, delta_gradient_w)]\n",
    "\n",
    "        # Обновление весов и смещений с использованием средних значений градиентов\n",
    "        self.weights = [w - (learning_rate / len(mini_batch)) * nw\n",
    "                        for w, nw in zip(self.weights, sum_gradient_w)]\n",
    "        self.biases = [b - (learning_rate / len(mini_batch)) * nb\n",
    "                       for b, nb in zip(self.biases, sum_gradient_b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9999404708753757\n"
     ]
    }
   ],
   "source": [
    "X = data_cls.drop(['bomb_planted_True'], axis=1)\n",
    "y = data_cls['bomb_planted_True'].values.reshape(-1, 1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Подготовка данных для MLP\n",
    "X_train = [X_train[i].reshape(-1, 1) for i in range(X_train.shape[0])]\n",
    "y_train = [y_train[i].reshape(-1, 1) for i in range(y_train.shape[0])]\n",
    "training_data = list(zip(X_train, y_train))\n",
    "\n",
    "# Определение параметров сети\n",
    "input_size = X.shape[1]\n",
    "hidden_layers = [10, 5]\n",
    "output_size = 1\n",
    "\n",
    "# Создание и обучение MLP\n",
    "mlp = MLP_cls([input_size] + hidden_layers + [output_size], ['relu'] * len(hidden_layers) + ['sigmoid'])\n",
    "learning_rate = 0.01\n",
    "for epoch in range(20):\n",
    "    np.random.shuffle(training_data)\n",
    "    mini_batches = [training_data[k:k+10] for k in range(0, len(training_data), 10)]\n",
    "    for mini_batch in mini_batches:\n",
    "        mlp.update_parameters(mini_batch, learning_rate)\n",
    "\n",
    "def predict(mlp, X):\n",
    "    predictions = [mlp.forward_propagation(x.reshape(-1, 1)) for x in X]\n",
    "    return np.array(predictions).squeeze()\n",
    "\n",
    "predictions = predict(mlp, X_test)\n",
    "predictions_binary = (predictions > 0.5).astype(int)\n",
    "accuracy = accuracy_score(y_test, predictions_binary)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_reg:\n",
    "    def __init__(self, layer_sizes, activation_functions):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activation_functions = activation_functions\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.activations = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]))\n",
    "            self.biases.append(np.random.randn(layer_sizes[i + 1]))\n",
    "            self.activations.append(np.zeros(layer_sizes[i + 1]))\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def derivative(self, x, activation_function):\n",
    "        if activation_function == 'sigmoid':\n",
    "            return x * (1 - x)\n",
    "        elif activation_function == 'tanh':\n",
    "            return 1 - np.power(x, 2)\n",
    "        elif activation_function == 'relu':\n",
    "            return (x > 0).astype(float)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.weights)):\n",
    "            x = np.dot(x, self.weights[i]) + self.biases[i]\n",
    "            if self.activation_functions[i] == 'sigmoid':\n",
    "                x = self.sigmoid(x)\n",
    "            elif self.activation_functions[i] == 'tanh':\n",
    "                x = self.tanh(x)\n",
    "            elif self.activation_functions[i] == 'relu':\n",
    "                x = self.relu(x)\n",
    "            self.activations[i] = x\n",
    "        return x\n",
    "\n",
    "    def backward(self, x, y, learning_rate):\n",
    "        output = self.forward(x)\n",
    "        deltas = []\n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            if i == len(self.weights) - 1:\n",
    "                error = output - y\n",
    "            else:\n",
    "                error = np.dot(deltas[-1], self.weights[i + 1].T)\n",
    "            delta = error * self.derivative(self.activations[i], self.activation_functions[i])\n",
    "            deltas.append(delta)\n",
    "        deltas.reverse()\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= learning_rate * np.dot(x.T if i == 0 else self.activations[i - 1].T, deltas[i])\n",
    "            self.biases[i] -= learning_rate * np.sum(deltas[i], axis=0)\n",
    "\n",
    "    def train(self, X, y, learning_rate, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            self.backward(X, y, learning_rate)\n",
    "\n",
    "    def r2_score(self, y_true, y_pred):\n",
    "        ss_res = np.sum(np.square(y_true - y_pred))\n",
    "        ss_tot = np.sum(np.square(y_true - np.mean(y_true)))\n",
    "        return 1 - ss_res / ss_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_score: 0.026932760330142358\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(trip_data):\n",
    "    X = data_reg.drop(columns=['quality']).to_numpy()\n",
    "    y = data_reg['quality'].to_numpy().reshape(-1, 1)\n",
    "    X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "    y = (y - y.mean()) / y.std()\n",
    "    return X, y\n",
    "\n",
    "# Подготовка данных\n",
    "X, y = preprocess_data(data_reg)\n",
    "\n",
    "# Инициализация и обучение модели\n",
    "layer_sizes = [X.shape[1], 10, 5, 1]  # Примерная архитектура сети\n",
    "activation_functions = ['tanh', 'relu', 'tanh']\n",
    "mlp = MLP_reg(layer_sizes, activation_functions)\n",
    "mlp.train(X, y, learning_rate=0.01, epochs=100)\n",
    "\n",
    "# Проверка модели\n",
    "y_pred = mlp.forward(X)\n",
    "r2 = mlp.r2_score(y, y_pred)\n",
    "print(f\"r2_score: {r2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
